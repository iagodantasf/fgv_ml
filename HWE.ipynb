{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3107ed",
   "metadata": {},
   "source": [
    "# Lista prática Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35702f0",
   "metadata": {},
   "source": [
    "**Instruções gerais:** Sua submissão deve conter: \n",
    "1. Um \"ipynb\" com seu código e as soluções dos problemas\n",
    "2. Uma versão pdf do ipynp\n",
    "\n",
    "**Dica:** Considere usar o Google Colab\n",
    "\n",
    "Essa lista terá o mesmo valor de uma questão na lista prática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cc6fb",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "Neste exercício, você implementará um MLP simples para classificação binária e explorará aspectos fundamentais do PyTorch que frequentemente causam confusão: gradientes acumulados, inicialização de pesos, e o comportamento do modo de avaliação vs treinamento.\n",
    "\n",
    "## Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configurar seed para reproducibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Gerar dataset toy\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converter para tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6b1a7",
   "metadata": {},
   "source": [
    "## Parte 1: Implementação Básica do MLP\n",
    "\n",
    "Implemente um MLP com a seguinte arquitetura:\n",
    "- Entrada: 2 features\n",
    "- Camada oculta 1: 16 neurônios, ativação ReLU\n",
    "- Camada oculta 2: 8 neurônios, ativação ReLU  \n",
    "- Saída: 1 neurônio (classificação binária com sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: Definir as camadas\n",
    "        # self.fc1 = ...\n",
    "        # self.fc2 = ...\n",
    "        # self.fc3 = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implementar o forward pass\n",
    "        # Importante: NÃO aplique sigmoid na saída aqui - explicaremos por quê\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefba13b",
   "metadata": {},
   "source": [
    "## Parte 2: O Problema dos Gradientes Acumulados\n",
    "\n",
    "Execute o código abaixo e explique por que os gradientes explodem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a640407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Loop de treino INCORRETO - encontre o bug\n",
    "for epoch in range(5):\n",
    "    for i in range(len(X_train)):\n",
    "        x = X_train[i:i+1]\n",
    "        y = y_train[i:i+1]\n",
    "        \n",
    "        # NÃO adicione optimizer.zero_grad() aqui propositalmente\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        loss = F.binary_cross_entropy(output, y.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            # Imprimir a norma do gradiente do primeiro peso\n",
    "            grad_norm = model.fc1.weight.grad.norm().item()\n",
    "            print(f\"Epoch {epoch}, Step {i}, Grad norm: {grad_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb8bda",
   "metadata": {},
   "source": [
    "\n",
    "**Questão 2.1**: Por que os gradientes crescem indefinidamente? Corrija o código.\n",
    "\n",
    "**Questão 2.2**: Onde exatamente você deve colocar `optimizer.zero_grad()` e `optimizer.step()`? Experimente colocar em lugares diferentes e observe o comportamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90968bbd",
   "metadata": {},
   "source": [
    "## Parte 3: Inicialização de Pesos\n",
    "\n",
    "Compare diferentes estratégias de inicialização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_zero(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.fill_(0.0)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "def init_weights_small(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "def init_weights_large(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.uniform_(-5, 5)\n",
    "\n",
    "# Teste cada inicialização\n",
    "init_functions = [init_weights_zero, init_weights_small, init_weights_large, None]\n",
    "init_names = ['zeros', 'small', 'large', 'default (Xavier)']\n",
    "\n",
    "for init_fn, name in zip(init_functions, init_names):\n",
    "    model = MLP()\n",
    "    if init_fn:\n",
    "        model.apply(init_fn)\n",
    "    \n",
    "    # TODO: Treinar por 10 épocas e plotar a loss\n",
    "    # Registre as activations da primeira camada oculta no início do treino\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden1 = F.relu(model.fc1(X_train[:100]))\n",
    "        print(f\"\\nInit {name}:\")\n",
    "        print(f\"  Activations mean: {hidden1.mean():.4f}\")\n",
    "        print(f\"  Activations std: {hidden1.std():.4f}\")\n",
    "        print(f\"  Dead neurons: {(hidden1 == 0).all(dim=0).sum().item()}/16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d9ed1",
   "metadata": {},
   "source": [
    "**Questão 3.1**: Por que a inicialização com zeros falha completamente?\n",
    "\n",
    "**Questão 3.2**: Calcule manualmente a variância das activations esperada para cada inicialização. Como isso se relaciona com o problema do vanishing/exploding gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6afe1f",
   "metadata": {},
   "source": [
    "## Parte 4: Modo de Treinamento vs Avaliação\n",
    "\n",
    "Adicione Dropout e BatchNorm ao modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd18827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithRegularization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPWithRegularization, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88ba43",
   "metadata": {},
   "source": [
    "Execute o seguinte experimento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPWithRegularization()\n",
    "\n",
    "# Passar o mesmo input 10 vezes SEM model.eval()\n",
    "model.train()\n",
    "outputs_train = []\n",
    "x_single = X_test[0:1]\n",
    "\n",
    "for _ in range(10):\n",
    "    outputs_train.append(model(x_single).item())\n",
    "\n",
    "print(\"Modo train - outputs:\", outputs_train)\n",
    "print(\"Desvio padrão:\", np.std(outputs_train))\n",
    "\n",
    "# Agora em modo eval\n",
    "model.eval()\n",
    "outputs_eval = []\n",
    "\n",
    "for _ in range(10):\n",
    "    outputs_eval.append(model(x_single).item())\n",
    "    \n",
    "print(\"\\nModo eval - outputs:\", outputs_eval)\n",
    "print(\"Desvio padrão:\", np.std(outputs_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d31d32",
   "metadata": {},
   "source": [
    "**Questão 4.1**: Por que os outputs variam em modo de treinamento mas são constantes em modo de avaliação?\n",
    "\n",
    "**Questão 4.2**: Implemente uma função que estime a incerteza do modelo usando Monte Carlo Dropout (manter dropout ativo durante a inferência):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87963c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, x, n_samples=100):\n",
    "    # TODO: Implementar predição com incerteza\n",
    "    # Dica: Force model.train() mesmo durante inferência\n",
    "    # Retorne média e desvio padrão das predições\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e54d5",
   "metadata": {},
   "source": [
    "## Parte 5: Investigando o Fluxo de Gradientes\n",
    "\n",
    "Implemente hooks para visualizar gradientes durante backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {}\n",
    "\n",
    "def save_gradient(name):\n",
    "    def hook(grad):\n",
    "        gradients[name] = grad\n",
    "    return hook\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "# Registrar hooks\n",
    "x = torch.randn(1, 2, requires_grad=True)\n",
    "x.register_hook(save_gradient('input'))\n",
    "\n",
    "# Forward e backward\n",
    "output = model(x)\n",
    "output.backward()\n",
    "\n",
    "# TODO: Adicionar hooks nas camadas intermediárias\n",
    "# Plotar a magnitude dos gradientes em cada camada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92b09b",
   "metadata": {},
   "source": [
    "**Questão 5.1**: Como a profundidade da rede afeta a magnitude dos gradientes? Experimente com redes de 2, 5 e 10 camadas.\n",
    "\n",
    "**Questão 5.2**: Implemente gradient clipping e mostre como ele previne exploding gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durante o loop de treino\n",
    "loss.backward()\n",
    "\n",
    "# TODO: Implementar gradient clipping\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
